{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NPI: Substantial Undocumented Infection Facilitates the Rapid Dissemination of Novel Coronavirus (SARS-CoV2)",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVU1Q900Dl2r"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors.\n",
        "##### Copyright 2020 Sen Pei (Columbia University).\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCZNiAlqEAr1"
      },
      "source": [
        "# Substantial Undocumented Infection Facilitates the Rapid Dissemination of Novel Coronavirus (SARS-CoV2)\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/probability/examples/Undocumented_Infection_and_the_Dissemination_of_SARS-CoV2\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Undocumented_Infection_and_the_Dissemination_of_SARS-CoV2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Undocumented_Infection_and_the_Dissemination_of_SARS-CoV2.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/probability/tensorflow_probability/examples/jupyter_notebooks/Undocumented_Infection_and_the_Dissemination_of_SARS-CoV2.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brvp_GH-ItsG"
      },
      "source": [
        "## Installation and Python Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUEZ07T3yuBj"
      },
      "source": [
        "!pip3 install -q tf-nightly tfp-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Za3hnj2y5M9"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import io\n",
        "import requests\n",
        "import time\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow_probability.python.internal import samplers\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfes = tfp.experimental.sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BovPI6LZK_Yn"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYfPl6uqzIs1"
      },
      "source": [
        "## Data Import\n",
        "\n",
        "Let's import the data from github and inspect some of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Py7pdKAAdjx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q22N4rPzXIg"
      },
      "source": [
        "raw_incidence = pd.read_csv('/content/drive/MyDrive/NPI/cases_final.csv', dtype=str)\n",
        "raw_mobility = pd.read_csv('/content/drive/MyDrive/NPI/mobility_final.csv', dtype=str)\n",
        "raw_population = pd.read_csv('/content/drive/MyDrive/NPI/population_final.csv', dtype=str)\n",
        "\n",
        "raw_incidence=raw_incidence.drop(columns=['Unnamed: 0'])\n",
        "raw_mobility=raw_mobility.drop(columns=['Unnamed: 0'])\n",
        "raw_population=raw_population.drop(columns=['Unnamed: 0'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFP-Tah4zfne"
      },
      "source": [
        "raw_incidence.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahwk3agwVKCb"
      },
      "source": [
        "raw_population.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMVX_5JPVn2M"
      },
      "source": [
        "raw_mobility['Day']=raw_mobility['Day'].astype(int)\r\n",
        "raw_mobility.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6iXSZK7Df5h"
      },
      "source": [
        "plt.plot(raw_incidence['01001'], '.-')\n",
        "plt.title('Wuhan incidence counts over 1/10/20 - 02/08/20')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS5vWXkBsQLG"
      },
      "source": [
        "IDX = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RWGcEfL65sS"
      },
      "source": [
        "And here we see the mobility matrix between different cities. This is a proxy for the number of people moving between different cities on the first 14 days.  It's dervied from GPS records provided by Tencent for the 2018 Lunar New Year season.  Li et al model mobility during the 2020 season as some unknown (subject to inference) constant factor $\\theta$ times this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvPonRBJ1AIQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSY98S14Oaqs"
      },
      "source": [
        "Finally, let's preprocess all this into numpy arrays that we can consume."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9txbadTC14YP"
      },
      "source": [
        "# The given populations are only \"initial\" because of intercity mobility during\n",
        "# the holiday season.\n",
        "initial_population = raw_population['Population'].to_numpy().astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-pKDK461zFX"
      },
      "source": [
        "Convert the mobility data into an [L, L, T]-shaped Tensor, where L is the number of locations, and T is the number of timesteps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-2zASNxw_wi"
      },
      "source": [
        "daily_mobility_matrices = []\n",
        "for i in range(1, 15):\n",
        "  day_mobility = raw_mobility[raw_mobility['Day'] == i]\n",
        "  print(i)\n",
        "  \n",
        "  # Make a matrix of daily mobilities.\n",
        "  z = pd.crosstab(\n",
        "      day_mobility.Origin, \n",
        "      day_mobility.Destination, \n",
        "      values=day_mobility['Mobility Index'], aggfunc='sum', dropna=False)\n",
        "  \n",
        "  # On a safer side, Include every county, even if there are no rows for some in the raw data on\n",
        "  # some day.  This uses the sort order of `raw_population`.\n",
        "  z = z.reindex(index=raw_population['fips'], columns=raw_population['fips'], \n",
        "                fill_value=0)\n",
        "  # Finally, fill any missing entries with 0. This means no mobility.\n",
        "  z = z.fillna(0)\n",
        "  daily_mobility_matrices.append(z.to_numpy())\n",
        "\n",
        "mobility_matrix_over_time = np.stack(daily_mobility_matrices, axis=-1).astype(\n",
        "    np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaJYQFT91-15"
      },
      "source": [
        "Finally take the observed infections and make an [L, T] table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs4TKTnv176D"
      },
      "source": [
        "# take the first 25 days.\n",
        "observed_daily_infectious_count = raw_incidence.to_numpy()[:14, 0:]\n",
        "observed_daily_infectious_count = np.transpose(observed_daily_infectious_count).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dymKgF8p1hgi"
      },
      "source": [
        "#Measuring against 3210 counties and for 25 days period\n",
        "print('Mobility Matrix over time should have shape (3210, 3210, 25): {}'.format(\n",
        "    mobility_matrix_over_time.shape))\n",
        "print('Observed Infectious should have shape (3210, 25): {}'.format(\n",
        "    observed_daily_infectious_count.shape))\n",
        "print('Initial population should have shape (3210,): {}'.format(\n",
        "    initial_population.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGTcVy2K2mxv"
      },
      "source": [
        "## Defining State and Parameters\n",
        "\n",
        "\n",
        "Let's start defining our model. The model we are reproducing is a variant of an [SEIR model](https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology#The_SEIR_model). In this case we have the following time-varying states:\n",
        "* $S$: Number of people susceptible to the disease in each city.\n",
        "* $E$: Number of people in each city exposed to the disease but not infectious yet.  Biologically, this corresponds to contracting the disease, in that all exposed people eventually become infectious.\n",
        "* $I^u$: Number of people in each city who are infectious but undocumented.  In the model, this actually means \"will never be documented\".\n",
        "* $I^r$: Number of people in each city who are infectious and documented as such.  Li et al model reporting delays, so $I^r$ actually corresponds to something like \"case is severe enough to be documented at some point in the future\".\n",
        "\n",
        "As we will see below, we will be inferring these states by running an Ensemble-adjusted Kalman Filter (EAKF) forward in time.  The state vector of the EAKF is one city-indexed vector for each of these quantities.\n",
        "\n",
        "The model has the following inferrable global, time-invariant parameters:\n",
        "\n",
        "* $\\beta$: The transmission rate due to documented-infectious individuals.\n",
        "* $\\mu$: The relative transmission rate due to undocumented-infectious\n",
        "  individuals. This will act through the product $\\mu \\beta$.\n",
        "* $\\theta$: The intercity mobility factor. This is a factor greater than\n",
        "  1 correcting for underreporting of mobility data (and for population growth\n",
        "  from 2018 to 2020).\n",
        "* $Z$: The average incubation period (i.e., time in the \"exposed\" state).\n",
        "* $\\alpha$: This is the fraction of infections severe enough to be (eventually) documented.\n",
        "* $D$: The average duration of infections (i.e., time in either \"infectious\" state).\n",
        "\n",
        "We will be inferring point estimates for these parameters with an Iterative-Filtering loop around the EAKF for the states.\n",
        "\n",
        "The model also depends on un-inferred constants:\n",
        "* $M$: The intercity mobility matrix.  This is time-varying and presumed given.  Recall that it's scaled by the inferred parameter $\\theta$ to give the actual population movements between cities.\n",
        "* $N$: The total number of people in each city.  The initial populations are taken as given, and the time-variation of population is computed from the mobility numbers $\\theta M$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--4jYF-Q9maG"
      },
      "source": [
        "First, we give ourselves some data structures for holding our states and parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VacihFdC25l4"
      },
      "source": [
        "SEIRComponents = collections.namedtuple(\n",
        "  typename='SEIRComponents',\n",
        "  field_names=[\n",
        "    'susceptible',              # S\n",
        "    'exposed',                  # E\n",
        "    'documented_infectious',    # I^r\n",
        "    'undocumented_infectious',  # I^u\n",
        "    # This is the count of new cases in the \"documented infectious\" compartment.\n",
        "    # We need this because we will introduce a reporting delay, between a person\n",
        "    # entering I^r and showing up in the observable case count data.\n",
        "    # This can't be computed from the cumulative `documented_infectious` count,\n",
        "    # because some portion of that population will move to the 'recovered'\n",
        "    # state, which we aren't tracking explicitly.\n",
        "    'daily_new_documented_infectious'])\n",
        "\n",
        "ModelParams = collections.namedtuple(\n",
        "    typename='ModelParams',\n",
        "    field_names=[\n",
        "      'documented_infectious_tx_rate',             # Beta\n",
        "      'undocumented_infectious_tx_relative_rate',  # Mu\n",
        "      'intercity_underreporting_factor',           # Theta\n",
        "      'average_latency_period',                    # Z\n",
        "      'fraction_of_documented_infections',         # Alpha\n",
        "      'average_infection_duration'                 # D\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBgUMzTL_2gc"
      },
      "source": [
        "We also code Li et al's bounds for the values of the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_HWdcXZ_1Cw"
      },
      "source": [
        "PARAMETER_LOWER_BOUNDS = ModelParams(\n",
        "    documented_infectious_tx_rate=0.8,\n",
        "    undocumented_infectious_tx_relative_rate=0.2,\n",
        "    intercity_underreporting_factor=1.,\n",
        "    average_latency_period=2.,\n",
        "    fraction_of_documented_infections=0.02,\n",
        "    average_infection_duration=2.\n",
        ")\n",
        "\n",
        "PARAMETER_UPPER_BOUNDS = ModelParams(\n",
        "    documented_infectious_tx_rate=1.5,\n",
        "    undocumented_infectious_tx_relative_rate=1.,\n",
        "    intercity_underreporting_factor=1.75,\n",
        "    average_latency_period=5.,\n",
        "    fraction_of_documented_infections=1.,\n",
        "    average_infection_duration=5.\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yizvLfw-CiT"
      },
      "source": [
        "## SEIR Dynamics\n",
        "\n",
        "Here we define the relationship between the parameters and state.\n",
        "\n",
        "The time-dynamics equations from Li et al (supplemental material, eqns 1-5) are as follows:\n",
        "\n",
        "$\\frac{dS_i}{dt} = -\\beta \\frac{S_i I_i^r}{N_i} - \\mu \\beta \\frac{S_i I_i^u}{N_i} + \\theta \\sum_k \\frac{M_{ij} S_j}{N_j - I_j^r} - + \\theta \\sum_k \\frac{M_{ji} S_j}{N_i - I_i^r}$\n",
        "\n",
        "$\\frac{dE_i}{dt} = \\beta \\frac{S_i I_i^r}{N_i} + \\mu \\beta \\frac{S_i I_i^u}{N_i} -\\frac{E_i}{Z} + \\theta \\sum_k \\frac{M_{ij} E_j}{N_j - I_j^r} - + \\theta \\sum_k \\frac{M_{ji} E_j}{N_i - I_i^r}$\n",
        "\n",
        "$\\frac{dI^r_i}{dt} = \\alpha \\frac{E_i}{Z} - \\frac{I_i^r}{D}$\n",
        "\n",
        "$\\frac{dI^u_i}{dt} = (1 - \\alpha) \\frac{E_i}{Z} - \\frac{I_i^u}{D} + \\theta \\sum_k \\frac{M_{ij} I_j^u}{N_j - I_j^r} - + \\theta \\sum_k \\frac{M_{ji} I^u_j}{N_i - I_i^r}$\n",
        "\n",
        "$N_i = N_i + \\theta \\sum_j M_{ij} - \\theta \\sum_j M_{ji}$\n",
        "\n",
        "As a reminder, the $i$ and $j$ subscripts index cities.  These equations model the time-evolution of the disease through\n",
        "- Contact with infectious individuals leading to more infection;\n",
        "- Disease progression from \"exposed\" to one of the \"infectious\" states;\n",
        "- Disease progression from \"infectious\" states to recovery, which we model by removal from the modeled population;\n",
        "- Inter-city mobility, including exposed or undocumented-infectious persons; and\n",
        "- Time-variation of daily city populations through inter-city mobility.\n",
        "\n",
        "Following Li et al, we assume that people with cases severe enough to eventually be reported do not travel between cities.\n",
        "\n",
        "Also following Li et al, we treat these dynamics as subject to term-wise Poisson noise, i.e., each term is actually the rate of a Poisson, a sample from which gives the true change.  The Poisson noise is term-wise because subtracting (as opposed to adding) Poisson samples does not yield a Poisson-distributed result.\n",
        "\n",
        "We will evolve these dynamics forward in time with the classic fourth-order Runge-Kutta integrator, but first let's define the function that computes them (including sampling the Poisson noise)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Cpj5Lyo-AGw"
      },
      "source": [
        "def sample_state_deltas(\n",
        "    state, population, mobility_matrix, params, seed, is_deterministic=False):\n",
        "  \"\"\"Computes one-step change in state, including Poisson sampling.\n",
        "  \n",
        "  Note that this is coded to support vectorized evaluation on arbitrary-shape\n",
        "  batches of states.  This is useful, for example, for running multiple\n",
        "  independent replicas of this model to compute credible intervals for the\n",
        "  parameters.  We refer to the arbitrary batch shape with the conventional\n",
        "  `B` in the parameter documentation below.  This function also, of course,\n",
        "  supports broadcasting over the batch shape.\n",
        "\n",
        "  Args:\n",
        "    state: A `SEIRComponents` tuple with fields Tensors of shape\n",
        "      B + [num_locations] giving the current disease state.\n",
        "    population: A Tensor of shape B + [num_locations] giving the current city\n",
        "      populations.\n",
        "    mobility_matrix: A Tensor of shape B + [num_locations, num_locations] giving\n",
        "      the current baseline inter-city mobility.\n",
        "    params: A `ModelParams` tuple with fields Tensors of shape B giving the\n",
        "      global parameters for the current EAKF run.\n",
        "    seed: Initial entropy for pseudo-random number generation.  The Poisson\n",
        "      sampling is repeatable by supplying the same seed.\n",
        "    is_deterministic: A `bool` flag to turn off Poisson sampling if desired.\n",
        "\n",
        "  Returns:\n",
        "    delta: A `SEIRComponents` tuple with fields Tensors of shape\n",
        "      B + [num_locations] giving the one-day changes in the state, according\n",
        "      to equations 1-4 above (including Poisson noise per Li et al).\n",
        "  \"\"\"\n",
        "  undocumented_infectious_fraction = state.undocumented_infectious / population\n",
        "  documented_infectious_fraction = state.documented_infectious / population\n",
        "\n",
        "  # Anyone not documented as infectious is considered mobile\n",
        "  mobile_population = (population - state.documented_infectious)\n",
        "  def compute_outflow(compartment_population):\n",
        "    raw_mobility = tf.linalg.matvec(\n",
        "        mobility_matrix, compartment_population / mobile_population)\n",
        "    return params.intercity_underreporting_factor * raw_mobility\n",
        "  def compute_inflow(compartment_population):\n",
        "    raw_mobility = tf.linalg.matmul(\n",
        "        mobility_matrix,\n",
        "        (compartment_population / mobile_population)[..., tf.newaxis],\n",
        "        transpose_a=True)\n",
        "    return params.intercity_underreporting_factor * tf.squeeze(\n",
        "        raw_mobility, axis=-1)\n",
        "\n",
        "  # Helper for sampling the Poisson-variate terms.\n",
        "  seeds = samplers.split_seed(seed, n=11)\n",
        "  if is_deterministic:\n",
        "    def sample_poisson(rate):\n",
        "      return rate\n",
        "  else:\n",
        "    def sample_poisson(rate):\n",
        "      return tfd.Poisson(rate=rate).sample(seed=seeds.pop())\n",
        "\n",
        "  # Below are the various terms called U1-U12 in the paper. We combined the\n",
        "  # first two, which should be fine; both are poisson so their sum is too, and\n",
        "  # there's no risk (as there could be in other terms) of going negative.\n",
        "  susceptible_becoming_exposed = sample_poisson(\n",
        "      state.susceptible *\n",
        "      (params.documented_infectious_tx_rate *\n",
        "       documented_infectious_fraction +\n",
        "       (params.undocumented_infectious_tx_relative_rate *\n",
        "        params.documented_infectious_tx_rate) *\n",
        "       undocumented_infectious_fraction))  # U1 + U2\n",
        "\n",
        "  susceptible_population_inflow = sample_poisson(\n",
        "      compute_inflow(state.susceptible))  # U3\n",
        "  susceptible_population_outflow = sample_poisson(\n",
        "      compute_outflow(state.susceptible))  # U4\n",
        "\n",
        "  exposed_becoming_documented_infectious = sample_poisson(\n",
        "      params.fraction_of_documented_infections *\n",
        "      state.exposed / params.average_latency_period)  # U5\n",
        "  exposed_becoming_undocumented_infectious = sample_poisson(\n",
        "      (1 - params.fraction_of_documented_infections) *\n",
        "      state.exposed / params.average_latency_period)  # U6\n",
        "\n",
        "  exposed_population_inflow = sample_poisson(\n",
        "      compute_inflow(state.exposed))  # U7\n",
        "  exposed_population_outflow = sample_poisson(\n",
        "      compute_outflow(state.exposed))  # U8\n",
        "\n",
        "  documented_infectious_becoming_recovered = sample_poisson(\n",
        "      state.documented_infectious /\n",
        "      params.average_infection_duration)  # U9\n",
        "  undocumented_infectious_becoming_recovered = sample_poisson(\n",
        "      state.undocumented_infectious /\n",
        "      params.average_infection_duration)  # U10\n",
        "\n",
        "  undocumented_infectious_population_inflow = sample_poisson(\n",
        "      compute_inflow(state.undocumented_infectious))  # U11\n",
        "  undocumented_infectious_population_outflow = sample_poisson(\n",
        "      compute_outflow(state.undocumented_infectious))  # U12\n",
        "\n",
        "  # The final state_deltas\n",
        "  return SEIRComponents(\n",
        "      # Equation [1]\n",
        "      susceptible=(-susceptible_becoming_exposed +\n",
        "                   susceptible_population_inflow +\n",
        "                   -susceptible_population_outflow),\n",
        "      # Equation [2]\n",
        "      exposed=(susceptible_becoming_exposed +\n",
        "               -exposed_becoming_documented_infectious +\n",
        "               -exposed_becoming_undocumented_infectious +\n",
        "               exposed_population_inflow +\n",
        "               -exposed_population_outflow),\n",
        "      # Equation [3]\n",
        "      documented_infectious=(\n",
        "          exposed_becoming_documented_infectious +\n",
        "          -documented_infectious_becoming_recovered),\n",
        "      # Equation [4]\n",
        "      undocumented_infectious=(\n",
        "          exposed_becoming_undocumented_infectious +\n",
        "          -undocumented_infectious_becoming_recovered +\n",
        "          undocumented_infectious_population_inflow +\n",
        "          -undocumented_infectious_population_outflow),\n",
        "      # New to-be-documented infectious cases, subject to the delayed\n",
        "      # observation model.\n",
        "      daily_new_documented_infectious=exposed_becoming_documented_infectious)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7T_QQTRKqOE"
      },
      "source": [
        "Here's the integrator.  This is completely standard, except for passing the PRNG seed through to the `sample_state_deltas` function to get independent Poisson noise at each of the partial steps that the Runge-Kutta method calls for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qx4tX8sCAZv"
      },
      "source": [
        "@tf.function(autograph=False)\n",
        "def rk4_one_step(state, population, mobility_matrix, params, seed):\n",
        "  \"\"\"Implement one step of RK4, wrapped around a call to sample_state_deltas.\"\"\"\n",
        "  # One seed for each RK sub-step\n",
        "  seeds = samplers.split_seed(seed, n=4)\n",
        "\n",
        "  deltas = tf.nest.map_structure(tf.zeros_like, state)\n",
        "  combined_deltas = tf.nest.map_structure(tf.zeros_like, state)\n",
        "\n",
        "  for a, b in zip([1., 2, 2, 1.], [6., 3., 3., 6.]):\n",
        "    next_input = tf.nest.map_structure(\n",
        "        lambda x, delta, a=a: x + delta / a, state, deltas)\n",
        "    deltas = sample_state_deltas(\n",
        "        next_input,\n",
        "        population,\n",
        "        mobility_matrix,\n",
        "        params,\n",
        "        seed=seeds.pop(), is_deterministic=False)\n",
        "    combined_deltas = tf.nest.map_structure(\n",
        "        lambda x, delta, b=b: x + delta / b, combined_deltas, deltas)\n",
        "\n",
        "  return tf.nest.map_structure(\n",
        "      lambda s, delta: s + tf.round(delta),\n",
        "      state, combined_deltas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gVKuTqi32Bn"
      },
      "source": [
        "## Initialization\n",
        "  \n",
        "Here we implement the initialization scheme from the paper.\n",
        "\n",
        "Following Li et al, our inference scheme will be an ensemble adjustment Kalman filter inner loop, surrounded by an iterated filtering outer loop (IF-EAKF).  Computationally, that means we need three kinds of initialization:\n",
        "- Initial state for the inner EAKF\n",
        "- Initial parameters for the outer IF, which are also the initial\n",
        "  parameters for the first EAKF\n",
        "- Updating parameters from one IF iteration to the next, which serve\n",
        "  as the initial parameters for each EAKF other than the first.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdRf_UmdvPLo"
      },
      "source": [
        "def initialize_state(num_particles, num_batches, seed):\n",
        "  \"\"\"Initialize the state for a batch of EAKF runs.\n",
        "  \n",
        "  Args:\n",
        "    num_particles: `int` giving the number of particles for the EAKF.\n",
        "    num_batches: `int` giving the number of independent EAKF runs to\n",
        "      initialize in a vectorized batch.\n",
        "    seed: PRNG entropy.\n",
        "    \n",
        "  Returns:\n",
        "    state: A `SEIRComponents` tuple with Tensors of shape [num_particles,\n",
        "      num_batches, num_cities] giving the initial conditions in each\n",
        "      city, in each filter particle, in each batch member.\n",
        "  \"\"\"\n",
        "  num_cities = mobility_matrix_over_time.shape[-2]\n",
        "  state_shape = [num_particles, num_batches, num_cities]\n",
        "  susceptible = initial_population * np.ones(state_shape, dtype=np.float32)\n",
        "  documented_infectious = np.zeros(state_shape, dtype=np.float32)\n",
        "  daily_new_documented_infectious = np.zeros(state_shape, dtype=np.float32)\n",
        "\n",
        "  # Following Li et al, initialize Wuhan with up to 2000 people exposed\n",
        "  # and another up to 2000 undocumented infectious.\n",
        "  rng = np.random.RandomState(seed[0] % (2**31 - 1))\n",
        "  wuhan_exposed = rng.randint(\n",
        "      0, 2001, [num_particles, num_batches]).astype(np.float32)\n",
        "  wuhan_undocumented_infectious = rng.randint(\n",
        "      0, 2001, [num_particles, num_batches]).astype(np.float32)\n",
        " \n",
        "  # Also following Li et al, initialize cities adjacent to Wuhan with three\n",
        "  # days' worth of additional exposed and undocumented-infectious cases,\n",
        "  # as they may have traveled there before the beginning of the modeling\n",
        "  # period.\n",
        "  exposed = 3 * mobility_matrix_over_time[\n",
        "      IDX, :, 0] * wuhan_exposed[\n",
        "          ..., np.newaxis] / initial_population[IDX]\n",
        "  undocumented_infectious = 3 * mobility_matrix_over_time[\n",
        "      IDX, :, 0] * wuhan_undocumented_infectious[\n",
        "          ..., np.newaxis] / initial_population[IDX]\n",
        "\n",
        "  exposed[..., IDX] = wuhan_exposed\n",
        "  undocumented_infectious[...,IDX] = wuhan_undocumented_infectious\n",
        "\n",
        "  # Following Li et al, we do not remove the inital exposed and infectious\n",
        "  # persons from the susceptible population.\n",
        "  return SEIRComponents(\n",
        "      susceptible=tf.constant(susceptible),\n",
        "      exposed=tf.constant(exposed),\n",
        "      documented_infectious=tf.constant(documented_infectious),\n",
        "      undocumented_infectious=tf.constant(undocumented_infectious),\n",
        "      daily_new_documented_infectious=tf.constant(daily_new_documented_infectious))\n",
        "  \n",
        "def initialize_params(num_particles, num_batches, seed):\n",
        "  \"\"\"Initialize the global parameters for the entire inference run.\n",
        "\n",
        "  Args:\n",
        "    num_particles: `int` giving the number of particles for the EAKF.\n",
        "    num_batches: `int` giving the number of independent EAKF runs to\n",
        "      initialize in a vectorized batch.\n",
        "    seed: PRNG entropy.\n",
        "    \n",
        "  Returns:\n",
        "    params: A `ModelParams` tuple with fields Tensors of shape\n",
        "      [num_particles, num_batches] giving the global parameters\n",
        "      to use for the first batch of EAKF runs.\n",
        "  \"\"\"\n",
        "  # We have 6 parameters. We'll initialize with a Sobol sequence,\n",
        "  # covering the hyper-rectangle defined by our parameter limits.\n",
        "  halton_sequence = tfp.mcmc.sample_halton_sequence(\n",
        "      dim=6, num_results=num_particles * num_batches, seed=seed)\n",
        "  halton_sequence = tf.reshape(\n",
        "      halton_sequence, [num_particles, num_batches, 6])\n",
        "  halton_sequences = tf.nest.pack_sequence_as(\n",
        "      PARAMETER_LOWER_BOUNDS, tf.split(\n",
        "          halton_sequence, num_or_size_splits=6, axis=-1))\n",
        "  def interpolate(minval, maxval, h):\n",
        "    return (maxval - minval) * h + minval\n",
        "  return tf.nest.map_structure(\n",
        "      interpolate,\n",
        "      PARAMETER_LOWER_BOUNDS, PARAMETER_UPPER_BOUNDS, halton_sequences)\n",
        "\n",
        "def update_params(num_particles, num_batches,\n",
        "                  prev_params, parameter_variance, seed):\n",
        "  \"\"\"Update the global parameters between EAKF runs.\n",
        "\n",
        "  Args:\n",
        "    num_particles: `int` giving the number of particles for the EAKF.\n",
        "    num_batches: `int` giving the number of independent EAKF runs to\n",
        "      initialize in a vectorized batch.\n",
        "    prev_params: A `ModelParams` tuple of the parameters used for the previous\n",
        "      EAKF run.\n",
        "    parameter_variance: A `ModelParams` tuple specifying how much to drift\n",
        "      each parameter.\n",
        "    seed: PRNG entropy.\n",
        "    \n",
        "  Returns:\n",
        "    params: A `ModelParams` tuple with fields Tensors of shape\n",
        "      [num_particles, num_batches] giving the global parameters\n",
        "      to use for the next batch of EAKF runs.\n",
        "  \"\"\"\n",
        "  # Initialize near the previous set of parameters. This is the first step\n",
        "  # in Iterated Filtering.\n",
        "  seeds = tf.nest.pack_sequence_as(\n",
        "      prev_params, samplers.split_seed(seed, n=len(prev_params)))\n",
        "  return tf.nest.map_structure(\n",
        "      lambda x, v, seed: x + tf.math.sqrt(v) * tf.random.stateless_normal([\n",
        "          num_particles, num_batches, 1], seed=seed),\n",
        "      prev_params, parameter_variance, seeds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKWm64gF0JkZ"
      },
      "source": [
        "## Delays\n",
        "\n",
        "One of the important features of this model is taking explicit account of the fact that infections are reported later than they begin.  That is, we expect that a person who moves from the $E$ compartment to the $I^r$ compartment on day $t$ may not show up in the observable reported case counts until some later day.\n",
        "\n",
        "We assume the delay is gamma-distributed.  Following Li et al, we use 1.85 for the shape, and parameterize the rate to produce an average reporting delay of 9 days."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFNy6L491BFD"
      },
      "source": [
        "def raw_reporting_delay_distribution(gamma_shape=1.85, reporting_delay=9.):\n",
        "  return tfp.distributions.Gamma(\n",
        "      concentration=gamma_shape, rate=gamma_shape / reporting_delay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQTwUeKr8dVP"
      },
      "source": [
        "Our observations are discrete, so we will round the raw (continuous) delays up to the nearest day.  We also have a finite data horizon, so the delay distribution for a single person is a categorical over the remaining days.  We can therefore compute the per-city predicted observations more efficiently than sampling $O(I^r)$ gammas, by pre-computing multinomial delay probabilities instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YRv0FjX9DfB"
      },
      "source": [
        "def reporting_delay_probs(num_timesteps, gamma_shape=1.85, reporting_delay=9.):\n",
        "  gamma_dist = raw_reporting_delay_distribution(gamma_shape, reporting_delay)\n",
        "  multinomial_probs = [gamma_dist.cdf(1.)]\n",
        "  for k in range(2, num_timesteps + 1):\n",
        "    multinomial_probs.append(gamma_dist.cdf(k) - gamma_dist.cdf(k - 1))\n",
        "  # For samples that are larger than T.\n",
        "  multinomial_probs.append(gamma_dist.survival_function(num_timesteps))\n",
        "  multinomial_probs = tf.stack(multinomial_probs)\n",
        "  return multinomial_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WAHi68lBFqz"
      },
      "source": [
        "Here's the code for actually applying these delays to the new daily documented infectious counts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrPHDvUEBPsS"
      },
      "source": [
        "def delay_reporting(\n",
        "    daily_new_documented_infectious, num_timesteps, t, multinomial_probs, seed):\n",
        "  # This is the distribution of observed infectious counts from the current\n",
        "  # timestep.\n",
        "\n",
        "  raw_delays = tfd.Multinomial(\n",
        "      total_count=daily_new_documented_infectious,\n",
        "      probs=multinomial_probs).sample(seed=seed)\n",
        "\n",
        "  # The last bucket is used for samples that are out of range of T + 1. Thus\n",
        "  # they are not going to be observable in this model.\n",
        "  clipped_delays = raw_delays[..., :-1]\n",
        "\n",
        "  # We can also remove counts that are such that t + i >= T.\n",
        "  clipped_delays = clipped_delays[..., :num_timesteps - t]\n",
        "  # We finally shift everything by t. That means prepending with zeros.\n",
        "  return tf.concat([\n",
        "      tf.zeros(\n",
        "          tf.concat([\n",
        "              tf.shape(clipped_delays)[:-1], [t]], axis=0),\n",
        "          dtype=clipped_delays.dtype),\n",
        "      clipped_delays], axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRsd3wsW9WYQ"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lO5MVT6M2lc"
      },
      "source": [
        "First we'll define some data structures for inference.\n",
        "\n",
        "In particular, we'll be wanting to do Iterated Filtering, which packages the\n",
        "state and parameters together while doing inference. So we'll define\n",
        "a `ParameterStatePair` object.\n",
        "\n",
        "We also want to package any side information to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JrCjAhXM6TL"
      },
      "source": [
        "ParameterStatePair = collections.namedtuple(\n",
        "    'ParameterStatePair', ['state', 'params'])\n",
        "\n",
        "# Info that is tracked and mutated but should not have inference performed over.\n",
        "SideInfo = collections.namedtuple(\n",
        "    'SideInfo', [\n",
        "        # Observations at every time step.\n",
        "        'observations_over_time',\n",
        "        'initial_population',\n",
        "        'mobility_matrix_over_time',\n",
        "        'population',\n",
        "        # Used for variance of measured observations.\n",
        "        'actual_reported_cases',\n",
        "        # Pre-computed buckets for the multinomial distribution.\n",
        "        'multinomial_probs',\n",
        "        'seed',\n",
        "    ])\n",
        "\n",
        "# Cities can not fall below this fraction of people\n",
        "MINIMUM_CITY_FRACTION = 0.6\n",
        "\n",
        "# How much to inflate the covariance by.\n",
        "INFLATION_FACTOR = 1.1\n",
        "\n",
        "INFLATE_FN = tfes.inflate_by_scaled_identity_fn(INFLATION_FACTOR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoZPQ9YDM9mj"
      },
      "source": [
        "Here is the complete observation model, packaged for the Ensemble Kalman Filter.\n",
        "\n",
        "The interesting feature is the reporting delays (computed as previously).  The upstream model emits the `daily_new_documented_infectious` for each city at each time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBrt1BkO9Xje"
      },
      "source": [
        "# We observe the observed infections.\n",
        "def observation_fn(t, state_params, extra):\n",
        "  \"\"\"Generate reported cases.\n",
        "  \n",
        "  Args:\n",
        "    state_params: A `ParameterStatePair` giving the current parameters\n",
        "      and state.\n",
        "    t: Integer giving the current time.\n",
        "    extra: A `SideInfo` carrying auxiliary information.\n",
        "\n",
        "  Returns:\n",
        "    observations: A Tensor of predicted observables, namely new cases\n",
        "      per city at time `t`.\n",
        "    extra: Update `SideInfo`.\n",
        "  \"\"\"\n",
        "  # Undo padding introduced in `inference`.\n",
        "  daily_new_documented_infectious = state_params.state.daily_new_documented_infectious[..., 0]\n",
        "  # Number of people that we have already committed to become\n",
        "  # observed infectious over time.\n",
        "  # shape: batch + [num_particles, num_cities, time]\n",
        "  observations_over_time = extra.observations_over_time\n",
        "  num_timesteps = observations_over_time.shape[-1]\n",
        "\n",
        "  seed, new_seed = samplers.split_seed(extra.seed, salt='reporting delay')\n",
        "  \n",
        "  daily_delayed_counts = delay_reporting(\n",
        "      daily_new_documented_infectious, num_timesteps, t,\n",
        "      extra.multinomial_probs, seed)\n",
        "  observations_over_time = observations_over_time + daily_delayed_counts\n",
        "\n",
        "  extra = extra._replace(\n",
        "      observations_over_time=observations_over_time,\n",
        "      seed=new_seed)\n",
        "\n",
        "  # Actual predicted new cases, re-padded.\n",
        "  adjusted_observations = observations_over_time[..., t][..., tf.newaxis]\n",
        "  # Finally observations have variance that is a function of the true observations:\n",
        "  return tfd.MultivariateNormalDiag(\n",
        "      loc=adjusted_observations,\n",
        "      scale_diag=tf.math.maximum(\n",
        "          2., extra.actual_reported_cases[..., t][..., tf.newaxis] / 2.)), extra"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugNrSlDAyUc6"
      },
      "source": [
        "Here we define the transition dynamics. We've done the semantic work already; here we just package it for the EAKF framework, and, following Li et al, clip city populations to prevent them from getting too small."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95S7LAj4NB_5"
      },
      "source": [
        "def transition_fn(t, state_params, extra):\n",
        "  \"\"\"SEIR dynamics.\n",
        "\n",
        "  Args:\n",
        "    state_params: A `ParameterStatePair` giving the current parameters\n",
        "      and state.\n",
        "    t: Integer giving the current time.\n",
        "    extra: A `SideInfo` carrying auxiliary information.\n",
        "\n",
        "  Returns:\n",
        "    state_params: A `ParameterStatePair` predicted for the next time step.\n",
        "    extra: Updated `SideInfo`.\n",
        "  \"\"\"\n",
        "  mobility_t = extra.mobility_matrix_over_time[..., t]\n",
        "  new_seed, rk4_seed = samplers.split_seed(extra.seed, salt='Transition')\n",
        "  new_state = rk4_one_step(\n",
        "      state_params.state,\n",
        "      extra.population,\n",
        "      mobility_t,\n",
        "      state_params.params,\n",
        "      seed=rk4_seed)\n",
        "\n",
        "  # Make sure population doesn't go below MINIMUM_CITY_FRACTION.\n",
        "  new_population = (\n",
        "      extra.population + state_params.params.intercity_underreporting_factor * (\n",
        "          # Inflow\n",
        "          tf.reduce_sum(mobility_t, axis=-2) - \n",
        "          # Outflow\n",
        "          tf.reduce_sum(mobility_t, axis=-1)))\n",
        "  new_population = tf.where(\n",
        "      new_population < MINIMUM_CITY_FRACTION * extra.initial_population,\n",
        "      extra.initial_population * MINIMUM_CITY_FRACTION,\n",
        "      new_population)\n",
        "\n",
        "  extra = extra._replace(population=new_population, seed=new_seed)\n",
        "\n",
        "  # The Ensemble Kalman Filter code expects the transition function to return a distribution.\n",
        "  # As the dynamics and noise are encapsulated above, we construct a `JointDistribution` that when\n",
        "  # sampled, returns the values above.\n",
        "\n",
        "  new_state = tfd.JointDistributionNamed(\n",
        "      model=tf.nest.map_structure(lambda x: tfd.VectorDeterministic(x), new_state))\n",
        "  params = tfd.JointDistributionNamed(\n",
        "      model=tf.nest.map_structure(lambda x: tfd.VectorDeterministic(x), state_params.params))\n",
        "  \n",
        "  state_params = tfd.JointDistributionNamed(\n",
        "      model=ParameterStatePair(state=new_state, params=params))\n",
        "\n",
        "  return state_params, extra"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsdNsvzuyaN-"
      },
      "source": [
        "Finally we define the inference method. This is two loops, the outer loop\n",
        "being Iterated Filtering while the inner loop is Ensemble Adjustment Kalman Filtering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUWinMkUNFGJ"
      },
      "source": [
        "# Use tf.function to speed up EAKF prediction and updates.\n",
        "ensemble_kalman_filter_predict = tf.function(\n",
        "    tfes.ensemble_kalman_filter_predict, autograph=False)\n",
        "ensemble_adjustment_kalman_filter_update = tf.function(\n",
        "    tfes.ensemble_adjustment_kalman_filter_update, autograph=False)\n",
        "\n",
        "def inference(\n",
        "    num_ensembles,\n",
        "    num_batches,\n",
        "    num_iterations,\n",
        "    actual_reported_cases,\n",
        "    mobility_matrix_over_time,\n",
        "    seed=None,\n",
        "    # This is how much to reduce the variance by in every iterative\n",
        "    # filtering step.\n",
        "    variance_shrinkage_factor=0.9,\n",
        "    # Days before infection is reported.\n",
        "    reporting_delay=9.,\n",
        "    # Shape parameter of Gamma distribution.\n",
        "    gamma_shape_parameter=1.85):\n",
        "  \"\"\"Inference for the Shaman, et al. model.\n",
        "\n",
        "  Args:\n",
        "    num_ensembles: Number of particles to use for EAKF.\n",
        "    num_batches: Number of batches of IF-EAKF to run.\n",
        "    num_iterations: Number of iterations to run iterative filtering.\n",
        "    actual_reported_cases: `Tensor` of shape `[L, T]` where `L` is the number\n",
        "      of cities, and `T` is the timesteps.\n",
        "    mobility_matrix_over_time: `Tensor` of shape `[L, L, T]` which specifies the\n",
        "      mobility between locations over time.\n",
        "    variance_shrinkage_factor: Python `float`. How much to reduce the\n",
        "      variance each iteration of iterated filtering.\n",
        "    reporting_delay: Python `float`. How many days before the infection\n",
        "      is reported.\n",
        "    gamma_shape_parameter: Python `float`. Shape parameter of Gamma distribution\n",
        "      of reporting delays.\n",
        "\n",
        "  Returns:\n",
        "    result: A `ModelParams` with fields Tensors of shape [num_batches],\n",
        "      containing the inferred parameters at the final iteration.\n",
        "  \"\"\"\n",
        "  print('Starting inference.')\n",
        "  num_timesteps = actual_reported_cases.shape[-1]\n",
        "  params_per_iter = []\n",
        "\n",
        "  multinomial_probs = reporting_delay_probs(\n",
        "      num_timesteps, gamma_shape_parameter, reporting_delay)\n",
        "\n",
        "  seed = samplers.sanitize_seed(seed, salt='Inference')\n",
        "\n",
        "  for i in range(num_iterations):\n",
        "    start_if_time = time.time()\n",
        "    seeds = samplers.split_seed(seed, n=4, salt='Initialize')\n",
        "    if params_per_iter:\n",
        "      parameter_variance = tf.nest.map_structure(\n",
        "          lambda minval, maxval: variance_shrinkage_factor ** (\n",
        "              2 * i) * (maxval - minval) ** 2 / 4.,\n",
        "          PARAMETER_LOWER_BOUNDS, PARAMETER_UPPER_BOUNDS)\n",
        "      params_t = update_params(\n",
        "          num_ensembles,\n",
        "          num_batches,\n",
        "          prev_params=params_per_iter[-1],\n",
        "          parameter_variance=parameter_variance,\n",
        "          seed=seeds.pop())\n",
        "    else:\n",
        "      params_t = initialize_params(num_ensembles, num_batches, seed=seeds.pop())\n",
        "\n",
        "    state_t = initialize_state(num_ensembles, num_batches, seed=seeds.pop())\n",
        "    population_t = sum(x for x in state_t)\n",
        "    observations_over_time = tf.zeros(\n",
        "        [num_ensembles,\n",
        "         num_batches,\n",
        "         actual_reported_cases.shape[0], num_timesteps])\n",
        "\n",
        "    extra = SideInfo(\n",
        "        observations_over_time=observations_over_time,\n",
        "        initial_population=tf.identity(population_t),\n",
        "        mobility_matrix_over_time=mobility_matrix_over_time,\n",
        "        population=population_t,\n",
        "        multinomial_probs=multinomial_probs,\n",
        "        actual_reported_cases=actual_reported_cases,\n",
        "        seed=seeds.pop())\n",
        "\n",
        "    # Clip states\n",
        "    state_t = clip_state(state_t, population_t)\n",
        "    params_t = clip_params(params_t, seed=seeds.pop())\n",
        "\n",
        "    # Accrue the parameter over time. We'll be averaging that\n",
        "    # and using that as our MLE estimate.\n",
        "    params_over_time = tf.nest.map_structure(\n",
        "        lambda x: tf.identity(x), params_t)\n",
        "\n",
        "    state_params = ParameterStatePair(state=state_t, params=params_t)\n",
        "\n",
        "    eakf_state = tfes.EnsembleKalmanFilterState(\n",
        "        step=tf.constant(0), particles=state_params, extra=extra)\n",
        "\n",
        "    for j in range(num_timesteps):\n",
        "      seeds = samplers.split_seed(eakf_state.extra.seed, n=3)\n",
        "      \n",
        "      extra = extra._replace(seed=seeds.pop())\n",
        "    \n",
        "      # Predict step.\n",
        "\n",
        "      # Inflate and clip.\n",
        "      new_particles = INFLATE_FN(eakf_state.particles)\n",
        "      state_t = clip_state(new_particles.state, eakf_state.extra.population)\n",
        "      params_t = clip_params(new_particles.params, seed=seeds.pop())\n",
        "      eakf_state = eakf_state._replace(\n",
        "          particles=ParameterStatePair(params=params_t, state=state_t))\n",
        "\n",
        "      eakf_predict_state = ensemble_kalman_filter_predict(eakf_state, transition_fn)\n",
        "\n",
        "      # Clip the state and particles.\n",
        "      state_params = eakf_predict_state.particles\n",
        "      state_t = clip_state(\n",
        "          state_params.state, eakf_predict_state.extra.population)\n",
        "      state_params = ParameterStatePair(state=state_t, params=state_params.params)\n",
        "\n",
        "      # We preprocess the state and parameters by affixing a 1 dimension. This is because for\n",
        "      # inference, we treat each city as independent. We could also introduce localization by\n",
        "      # considering cities that are adjacent.\n",
        "      state_params = tf.nest.map_structure(lambda x: x[..., tf.newaxis], state_params)\n",
        "      eakf_predict_state = eakf_predict_state._replace(particles=state_params)\n",
        "\n",
        "      # Update step.\n",
        "      \n",
        "      eakf_update_state = ensemble_adjustment_kalman_filter_update(\n",
        "          eakf_predict_state,\n",
        "          actual_reported_cases[..., j][..., tf.newaxis],\n",
        "          observation_fn)\n",
        "      \n",
        "      state_params = tf.nest.map_structure(\n",
        "          lambda x: x[..., 0], eakf_update_state.particles)\n",
        "\n",
        "      # Clip to ensure parameters / state are well constrained.\n",
        "      state_t = clip_state(\n",
        "          state_params.state, eakf_update_state.extra.population)\n",
        "      \n",
        "      # Finally for the parameters, we should reduce over all updates. We get\n",
        "      # an extra dimension back so let's do that.\n",
        "      params_t = tf.nest.map_structure(\n",
        "          lambda x, y: x + tf.reduce_sum(y[..., tf.newaxis] - x, axis=-2, keepdims=True),\n",
        "          eakf_predict_state.particles.params, state_params.params)\n",
        "      params_t = clip_params(params_t, seed=seeds.pop())\n",
        "      params_t = tf.nest.map_structure(lambda x: x[..., 0], params_t)\n",
        "\n",
        "      state_params = ParameterStatePair(state=state_t, params=params_t)\n",
        "      eakf_state = eakf_update_state\n",
        "      eakf_state = eakf_state._replace(particles=state_params)\n",
        "\n",
        "      # Flatten and collect the inferred parameter at time step t.\n",
        "      params_over_time = tf.nest.map_structure(\n",
        "          lambda s, x: tf.concat([s, x], axis=-1), params_over_time, params_t)\n",
        "\n",
        "    est_params = tf.nest.map_structure(\n",
        "        # Take the average over the Ensemble and over time.\n",
        "        lambda x: tf.math.reduce_mean(x, axis=[0, -1])[..., tf.newaxis],\n",
        "        params_over_time)\n",
        "    params_per_iter.append(est_params)\n",
        "    print('Iterated Filtering {} / {} Ran in: {:.2f} seconds'.format(\n",
        "        i, num_iterations, time.time() - start_if_time))\n",
        "\n",
        "  return tf.nest.map_structure(\n",
        "      lambda x: tf.squeeze(x, axis=-1), params_per_iter[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvALK7e1fgKU"
      },
      "source": [
        "Final detail: clipping the parameters and state consists of making sure they are within range, and non-negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQj7o0nU6-ru"
      },
      "source": [
        "def clip_state(state, population):\n",
        "  \"\"\"Clip state to sensible values.\"\"\"\n",
        "  state = tf.nest.map_structure(\n",
        "      lambda x: tf.where(x < 0, 0., x), state)\n",
        "\n",
        "  # If S > population, then adjust as well.\n",
        "  susceptible = tf.where(state.susceptible > population, population, state.susceptible)\n",
        "  return SEIRComponents(\n",
        "      susceptible=susceptible,\n",
        "      exposed=state.exposed,\n",
        "      documented_infectious=state.documented_infectious,\n",
        "      undocumented_infectious=state.undocumented_infectious,\n",
        "      daily_new_documented_infectious=state.daily_new_documented_infectious)\n",
        "\n",
        "def clip_params(params, seed):\n",
        "  \"\"\"Clip parameters to bounds.\"\"\"\n",
        "  def _clip(p, minval, maxval):\n",
        "    return tf.where(\n",
        "        p < minval,\n",
        "        minval * (1. + 0.1 * tf.random.stateless_uniform(p.shape, seed=seed)),\n",
        "        tf.where(p > maxval,\n",
        "                 maxval * (1. - 0.1 * tf.random.stateless_uniform(\n",
        "                     p.shape, seed=seed)), p))\n",
        "  params = tf.nest.map_structure(\n",
        "      _clip, params, PARAMETER_LOWER_BOUNDS, PARAMETER_UPPER_BOUNDS)\n",
        "\n",
        "  return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7ACJedFAyHQ"
      },
      "source": [
        "## Running it all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T603NTSA0f3"
      },
      "source": [
        "# Let's sample the parameters.\n",
        "#\n",
        "# NOTE: Li et al. run inference 1000 times, which would take a few hours.\n",
        "# Here we run inference 30 times (in a single, vectorized batch).\n",
        "best_parameters = inference(\n",
        "    num_ensembles=300,\n",
        "    num_batches=30,\n",
        "    num_iterations=10,\n",
        "    actual_reported_cases=observed_daily_infectious_count,\n",
        "    mobility_matrix_over_time=mobility_matrix_over_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWjY1KpJ3JuT"
      },
      "source": [
        "The results of our inferences.  We plot the maximum-likelihood values for all the global paramters to show their variation across our `num_batches` independent runs of inference.  This corresponds to Table S1 in the supplemental materials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZqHM3nIQ8Zj"
      },
      "source": [
        "fig, axs = plt.subplots(2, 3)\n",
        "axs[0, 0].boxplot(best_parameters.documented_infectious_tx_rate,\n",
        "                  whis=(2.5,97.5), sym='')\n",
        "axs[0, 0].set_title(r'$\\beta$')\n",
        "\n",
        "axs[0, 1].boxplot(best_parameters.undocumented_infectious_tx_relative_rate,\n",
        "                  whis=(2.5,97.5), sym='')\n",
        "axs[0, 1].set_title(r'$\\mu$')\n",
        "\n",
        "axs[0, 2].boxplot(best_parameters.intercity_underreporting_factor,\n",
        "                  whis=(2.5,97.5), sym='')\n",
        "axs[0, 2].set_title(r'$\\theta$')\n",
        "\n",
        "axs[1, 0].boxplot(best_parameters.average_latency_period,\n",
        "                  whis=(2.5,97.5), sym='')\n",
        "axs[1, 0].set_title(r'$Z$')\n",
        "\n",
        "axs[1, 1].boxplot(best_parameters.fraction_of_documented_infections,\n",
        "                  whis=(2.5,97.5), sym='')\n",
        "axs[1, 1].set_title(r'$\\alpha$')\n",
        "\n",
        "axs[1, 2].boxplot(best_parameters.average_infection_duration,\n",
        "                  whis=(2.5,97.5), sym='')\n",
        "axs[1, 2].set_title(r'$D$')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC_vu5sekpx6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}